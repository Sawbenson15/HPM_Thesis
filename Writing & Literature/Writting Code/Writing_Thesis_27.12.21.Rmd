---
title: An Emperical Economic Investigation into COVID-19's Impact on Fundemental Hedonic
  Features in Real Estate Valuation Models
author: "Sawyer M. Benson"
date: "December 21, 2021"
output:
  html_notebook: default
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
bibliography: library.bib
link-citations: yes
---

<br><br>

### Cover Page

Under construction

<br>

### Abstract

Under construction

<br>

###     Table of Contents

1. **Introduction**
2. **Literature Review**
3. **Data**
4. **Methodology**
5. **Results**
6. **Discussion**
7. **Conclusion**
8. **Bibliography**
9. **Appendix**

<br>

### Figures, Tables, and Abbreviations
Under construction

<br><br>

### 1. Introduction

In late 2019, a virus first detected in Wuhan, China would set in motion a global pandemic which, by the end of 2021, will have
killed 4.3 million people, infecting 238 million others, and disrupting the global economy across virtually every measurable
dimension [@WHODeath]. According to seven economic impact models constructed by @McKibbin, estimates of the total global economic
loss in terms of GDP are measured to be as large as 9.2 trillion U.S. dollars. However, despite a generally positive correlation between GDP and real estate prices *(see image 1)*, historically measured as high as 98%, real estate market prices in the U.S. hit near-record heights following the outbreak of COVID-19 [@AsiaGreen]. According to a report released by Zillow
Analytics [@Manhertz], U.S. real estate gained 2.5 trillion dollars of value in 2020 alone, representing the largest single-year
growth since 2005, despite an approximately 760 billion dollar decrease in GDP in the same year [@FRED2021]. The focus of this
thesis is to investigate the reasons behind why and how real estate market prices have broken trend and behaved so
uncharacteristically counter cyclical in the face of a global pandemic.

<br>
<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/GDP_Housing_Nominal.png){width=70%}
</center>
<br>

In the following sections, I will apply the Hedonic Pricing Method (HPM) to Louisiana housing market data in order to inferentially describe the economic impact of the global pandemic on residential housing market values. Furthermore, I will take advantage of the HPM's structural framework of using real estate properties' hedonic features (e.g., size, age, number of bedrooms, etc.) to test for changes in demand for specific property features pre vs post pandemic. The HPM will be econometrically modeled using an Ordinary Least Squares (OLS) regressions framework for specific variable analysis while several variations of machine learning (ML) prediction models will be estimated to test different independent variables' maximum explanatory power in predicting out-of-sample observations. The results of these models will shed light into the otherwise counter intuitive response of real estate pricing dynamics to the COVID-19 global pandemic.    

<br><br>

### 2. Literature Review

#### 2.1 Background

The market value of a commodity is most often theoretically defined as the equilibrium price derived from the basic economic
principal, or law, of supply and demand [@Locke; @Epple]. However, the real estate market often violates this assumption due to
its unique characteristics as an asset class [@Wheaton]. For example, much of the underlying utility of a property is its use as a means of shelter
by its owner [@Ling]. This rather unusual relationship to this asset introduces several behavioral biases which cause economic
frictions not accounted for by traditional neoclassical economic theory [@Nicolaides]. A notable example of behavioral bias impacting real estate price dynamics is the endowment effect. This behavioral finding was originally established by
@Kahneman in the late 20th century, and later applied to real estate markets by @BAO. The latter of the two stating that the predictably
irrational behavior of market participants to overvalue their home due to sentimental attachment to the property forces market prices into sustained economic disequilibrium. Other highly cited and unusual characteristics are that real
estate assets are very infrequently traded due to high transaction costs [@Collett; @Guilkey], governments tend to interfere, both
directly and indirectly with real estate markets through the creation of fiscal and monetary policies [@Bingyang; @Du], and through
creating renter-protections laws such as 'squatter's rights' laws which allow a renter to remain in a home for extended
periods of time long after they have stopped paying rent [@Hoy; @Gardiner].

#### 2.2 Real Estate Valuation Methods

The idiosyncratic asset features outlined in section 2.1 along with a high level of heterogeneity across many dimensions of the entire real estate asset class makes the creation of a generalized pricing model difficult and have led to a wide range of proposals and recommendations about what determines the market price of real estate assets and how to reliably model those pricing dynamics [@Curcuru]. In @Pagourtzi, the authors outlines several of the currently accepted real estate valuation methods, ranging from what they categorize as the *traditional methods*, such as comparable-group, cost, income-multiple, profit-multiple, and contractor’s method, to the *advanced methods*, such as ANNs, spatial analysis methods, fuzzy logic, and the hedonic pricing method. According to a meta analysis conducted by @Sirmans, currently, the most widely used and accepted advanced methodological framework for real estate valuation modeling is the Hedonic Pricing Method.


##### 2.2.1 Hedonic Prcing Model in Real Estate

First applied in 1939 on automobile data, according to @Goodman, the HPM is a model which estimates the value of distinct characteristics of a commodity which directly or indirectly contribute to its market value. Besides its implementation in real estate finance and economics, such as in this thesis, this methodology has a wide range of applications such as its implementation in consumer and market research [@Holbrook; @Arnold], construction of consumer price indices [@Moulton; @Schultze], various tax assessments [@Berry; @Bernasconi], automated automobile valuation [@Crowling; @Matas], and computer sales [@Dulberger; @Wakefield].

Since its introduction, the HPM has gain significant popularity among housing market and commercial real
estate researchers. The specific real estate-based topics include, but are not limited to, the construction of housing price indices [@Gourieroux; @Wallace], the estimation and prediction of a property's market value in situations where market-transaction data is low-dimensional or non-existent
[@LeSage], and, as in this thesis, the specific analysis of changes in the demand for specific property characteristics across time, subgroups, or
both [@Clapp]. As the broad search for a satisfactory modeling framework focuses in on the HPM, another debate arises regarding the best functional form of this method. Traditionally utilizing the standard OLS framework [@Pace], researchers are increasingly utilizing a variety machine learning algorithms to accomplish an increasingly more refined set of findings.

##### 2.2.2 OLS and the Hedonic Pricing Model in Real Estate
Unsurprisingly, regression analysis is the preferred estimation approach among real estate researchers when using HPT for price estimation. These multiple regression analysis methods are most often either an Ordinary Least Squares (OLS) regression or a Maximum Likelihood approximation of the log-likely equation derived directly from the hedonic function. Each of these estimation methods take a functionally similar path as they both estimate a vector of parameters (i.e. beta coefficients) that best fits the explanatory hedonic variables to the associated market price. They differ only by the loss function used in the identification of that best-fitted parameter vector.

The most commonly used hedonic price regression equation with respect to real estate markets models the relationship between market rents or market property values to a list of hedonic characteristics. The classical construction of this model according to **(Herath, S. K. & Maier, G. (2010))** is the following:

$$
R = f(P,N,L,t)
$$
where $R$ is rent or price of the property; $P$ is property related attributes; $N$ is neighborhood characteristics; $L$ is locational variables and; $t$ is an indicator of time.

##### 2.2.3 Machine Learning and the Hedonic Pricing Model in Real Estate

Though first introduced by @Turing under the broader umbrella term of *artificial intelligence*, the adoption of ML methods in real estate would take many years of software and hardware development, allowing or the subsequent collection of ever-larger data sets and central processing unites (CPUs) capable of processing the often extraordinary number of calculation required to produce a solution for a given algorithm [@Dutta]. The primary advantage of ML techniques are that ML algorithms learn and improve over time and across many iterations and variable combinations, while tradition statistical and econometric techniques produce static results across a single model [@Anguita].  
 
@Mohd provides a thorough overview of the various applications of ML to real estate valuation methods, including the Ridge and Lasso regression techniques used in this thesis.

**Work in Progress: Add more volume here. Possible addition of models?**

<br>

#### 2.3 COVID-19 Crisis' Impact on Real Estate Markets

In the wake of the COVID-19 crisis, there were several papers and articles regarding the economic impact of the global pandemic on the housing market being expeditiously publishes in virtually every major journal. In this section (2.3), I have selected the most relevant of these publications with respect to this thesis.

Structural and temporal changes in the housing market using hedonic methods [@Shimizu], 

Changes in housing market demand for specific property types and features [@Tajani],  

Potential changes in housing preferences due to the COVID-19 pandemic and highlight the challenges for policy making [@Nanda]



<br>

**Tonia Notes:**

* Add multiple sources on some citations
* Add real estate market during the crisis
* Break into subsections from widest to most specific
   + Make last subsection specific to this thesis, quoting sources and their findings
* Meta analysis is good



<br><br>

### 3. Data 

#### 3.1 Data Collection

The utilization of Big Data collected through a data-mining process called *web-scrapping* has increasingly become the method of choice for researchers across disciplines. The term web-scrapping simply refers to the process of collecting structured data from websites using algorithms to automate the collection process. Methods similar to ones I have implemented in this thesis have been used by established authors such as **Borde et al., Pérez-Rave et al. and Berawi et al.** 

In this thesis, I have used a mixture of the programming languages R and Python, supplemented by several packages created by Selenium, to write an algorithm that collects the required hedonic variables for this research from the Multiple Listing Services (MLS). **Table 1** is a summary of the original data set's key features.

<br>

```{r load-packages, include=FALSE}
library(dplyr)
library(magrittr)
library(knitr)
library(knitr)
#library(kableExtra)
library(readxl) # Import excel data frames
library(tidyverse)
library(dplyr)
library(tinytex)
library(readxl) # Import excel data frames
library(ggplot2) # Graphs
library(scales) # Scale range of ggplots 
library(ggfortify) # Additional ggplot2 functionality
library(olsrr) # Testing for heteroscedasticity
library(lmtest) # Testing for heteroscedasticity using breuch-pagan
library(sandwich) # Amending heteroskedasticity 
library(mcvis) # Visualizing multicollinearity
library(gridExtra) # Organize graphs
library(dplyr) # data_factor wrangling
library(tidyr) # data_factor wrangling
library(tinytex) #for RMarkdown
library(openxlsx) #Export data frame into Excel
library(ggeffects) # plotting marginal effects
library(sjPlot) # plotting marginal effects
library(stargazer) # Showing several outputs next to each other in a STATA style
library(modelsummary) # Showing several outputs next to each other in a STATA style
library(regclass) # for testing multicollinearity using VIF
library(jtools) # cleaner regression output (e.g. summ(lm) 
library(tidyverse) # data cleaning
library(hrbrthemes) # special boxplots
library(viridis) # special boxplots
library(plotly) # For 3D plotting in ggplot2
library(reshape2) # Corr heatmap
library(gt)


#detach("package:kableExtra", unload=TRUE)
#detach("package:knitr", unload=TRUE)

# Import and attach data sets
data_factor <- read_excel("/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Models/Data/New Data/3. data_factor_cleaned.xlsx")
attach(data_factor)

# Convert Char to Factors with N Levels
# Structure Change
data_factor$property_type <- as.factor(data_factor$property_type)
data_factor$ac_type <- as.factor(data_factor$ac_type)
data_factor$patio <- as.factor(data_factor$patio)
data_factor$school_general <- as.factor(data_factor$school_general)
data_factor$pool <- as.factor(data_factor$pool)
data_factor$roof_type <- as.factor(data_factor$roof_type)
data_factor$gas_type <- as.factor(data_factor$gas_type)
data_factor$out_building <- as.factor(data_factor$out_building)

data_factor$appliances <- as.factor(data_factor$appliances)
data_factor$garage <- as.factor(data_factor$garage)
data_factor$property_condition <- as.factor(data_factor$property_condition)
data_factor$energy_efficient <- as.factor(data_factor$energy_efficient)
data_factor$exterior_type <- as.factor(data_factor$exterior_type)
data_factor$exterior_features <- as.factor(data_factor$exterior_features)
data_factor$fireplace <- as.factor(data_factor$fireplace)
data_factor$foundation_type <- as.factor(data_factor$foundation_type)
data_factor$beds_total <- as.factor(data_factor$beds_total)
data_factor$bath_full <- as.factor(data_factor$bath_full)
data_factor$bath_half <- as.factor(data_factor$bath_half)
data_factor$sewer_type <- as.factor(data_factor$sewer_type)
data_factor$property_style <- as.factor(data_factor$property_style)
data_factor$subdivision <- as.factor(data_factor$subdivision)
data_factor$water_type <- as.factor(data_factor$water_type)
data_factor$waterfront <- as.factor(data_factor$waterfront)
data_factor$sold_date <- openxlsx::convertToDate(data_factor$sold_date)
data_factor$sold_date <- as.numeric(data_factor$sold_date)

str(data_factor)

# Splits
data_factor$city_limits <- as.factor(data_factor$city_limits)
data_factor$corona_date_split <- as.factor(data_factor$corona_date_split)
data_factor$top25_sold_price <- as.factor(data_factor$top25_sold_price)
data_factor$bottom25_sold_price <- as.factor(data_factor$bottom25_sold_price)
data_factor$top25_area_living <- as.factor(data_factor$top25_area_living)
data_factor$bottom25_area_living  <- as.factor(data_factor$bottom25_area_living)
data_factor$top25_age <- as.factor(data_factor$top25_age)
data_factor$bottom25_age <- as.factor(data_factor$bottom25_age)
data_factor$top25_dom <- as.factor(data_factor$top25_dom)
data_factor$bottom25_dom <- as.factor(data_factor$bottom25_dom)
data_factor$infections_period <- as.numeric(data_factor$infections_accum > 1000)
data_factor$infections_period <- as.factor(data_factor$infections_period)

str(data_factor)

# Remove this weird '20' level is bath_full
levels(data_factor$bath_full)
is.na(data_factor$bath_full) <- data_factor$bath_full == "20"
data_factor$bath_full <- factor(data_factor$bath_full)
levels(data_factor$bath_full)

# Remove beds_total > 5
levels(data_factor$beds_total)
is.na(data_factor$beds_total) <- data_factor$beds_total == "7" 
data_factor$beds_total <- factor(data_factor$beds_total)
is.na(data_factor$beds_total) <- data_factor$beds_total == "6" 
data_factor$beds_total <- factor(data_factor$beds_total)
levels(data_factor$beds_total)



levels(data_factor$beds_total)
levels(data_factor$bath_full)
levels(data_factor$bath_half)

# Data frame without Split Vars
names(data_factor)
data_factor_core <- data_factor[-c(36:47)]
data_factor_core <- subset(data_factor_core, select = -c(city_limits, mls_number, infections_period))
str(data_factor_core)
names(data_factor_core)



```

```{r echo=FALSE, results='asis'}

library(readxl) # Import excel data frames
df1 <- suppressMessages(read_excel("/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Tables/Table_Data_Gen_Diminsions.xlsx"))
df1 <- df1[,1:2]
colnames(df1) <- c("Name", "Information")


table1 <- gt(df1)
table1 <- table1 %>% tab_header(title = md("Variable List"),
                                subtitle = md("*Structure and short discription*"))
table1
```
<br>

```{r echo=FALSE, results='asis'}
# Table 2
df2 <- suppressMessages(read_excel("/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Tables/table_data_dimensions_original.xlsx"))
df2 <- df2[,1:3]
colnames(df2) <- c("Variable Type", "Variables", "Observations")

table2 <- gt(df2)
table2 <- table2 %>% tab_header(title = md("Variable List"),
                                subtitle = md("*Structure and short discription*"))
table2

# Joining Code
# Under construction

```

#### 3.2 Data Processing

Though the data-collecting algorithms return structured data, it is nevertheless far from being suitable for the rather picky
models which will eventually analyze them. Therefore, the following processes were completed in order to render the raw data
into a usable form:

1. **Missing values** (i.e. N/A values) were removed 
2. **Outliers** were identified and removed for all continuous variables
   + An 'outlier' is defined by being more than 1.5 standard deviations from the mean of the variable's own distribution
3. **Multilevel factor** data was broken out by each level into binary representations through *Hot-One* coding
   + Some features which had a large number of factor levels were simplified using Lasso regressive methods
4. **High-leverage** point observations, according to diagnostic linear regression, were removed
5. **Duplicates**, defined by the MLS unique identification number, were removed
6. **Structural errors** (e.g. dates structured as string variable) were corrected
7. **Binary variables** were created for key variables (e.g. city_limits) by the following standard method:

$$I\left(y\right)=\left\{\begin{array}{ll}1,\quad x\in A\\0, \quad x \notin A\end{array}\right.,$$
Where $I$ is an indicator function with space $A$ that composes dummy variable $x$ into $1$ if the condition is met and into $0$ if it is not.

<br>

The results of the data cleaning processes cane be seen in **table 2**.

```{r echo=FALSE, results='asis'}
# Cleaned Data Set Summary
df1 <- suppressMessages(read_excel("/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Tables/Table_Data_Gen_Diminsions.xlsx"))

table1 <- gt(df1)
table1 <- table1 %>% tab_header(title = md("Variable List"),
                                subtitle = md("*Structure and short discription*"))
table1
```

<br>

```{r echo=FALSE, results='asis'}
# Cleaned Data Structure Overview
df2 <- suppressMessages(read_excel("/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Tables/table_data_dimensions_original.xlsx"))
df2 <- df2 %>% select(1, 4:5)

colnames(df2) <- c("Variable Type", "Variables", "Observations")

table2 <- gt(df2)
table2 <- table2 %>% tab_header(title = md("Variable List"),
                                subtitle = md("*Structure and short discription*"))
table2

```



#### 3.3 Variable List

```{r echo=FALSE, results='asis'}
# Cleaned Data Set Summary
df3 <- suppressMessages(read_excel("/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Tables/Variable_table.xlsx"))

table1 <- gt(df3)
table1 <- table1 %>% tab_header(title = md("Variable List"),
                                subtitle = md("*Structure and short discription*"))

table1
```

<br><br>

#### 3.3 Data Distriptive Statistics

```{r include=FALSE}
library(readxl) # Import excel data frames
library(ggplot2) # Graphs
library(scales) # Scale range of ggplots 
library(ggfortify) # Additional ggplot2 functionality
library(olsrr) # Testing for heteroscedasticity
library(lmtest) # Testing for heteroscedasticity using breuch-pagan
library(sandwich) # Amending heteroskedasticity 
library(mcvis) # Visualizing multicollinearity
library(gridExtra) # Organize graphs
library(dplyr) # data_factor wrangling
library(tidyr) # data_factor wrangling
library(tinytex) #for RMarkdown
library(openxlsx) #Export data frame into Excel
library(ggeffects) # plotting marginal effects
library(sjPlot) # plotting marginal effects
library(stargazer) # Showing several outputs next to each other in a STATA style
library(modelsummary) # Showing several outputs next to each other in a STATA style
library(regclass) # for testing multicollinearity using VIF
library(jtools) # cleaner regression output (e.g. summ(lm) 
library(tidyverse) # data cleaning
library(hrbrthemes) # special boxplots
library(viridis) # special boxplots
library(plotly) # For 3D plotting in ggplot2
library(reshape2) # Corr heatmap

# Colors
very_low <- "#460f5c"
low <- "#2c728e"
med <- "#27ad81"
high <- "#f4e61e"

# Import and attach data sets
# Upload and process data_factor from OLS_Thesis

```

##### 3.3.1 Correlation 

The correlation matrix between all numeric variables shows that with exception to the variables which will have obvious correlations (e.g. sold_price and list_price, area_total and area_living, and infections figures), there are no other correlations which would cause concern.

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/correlation_matrix.png){width=80%}
</center>

<br>

##### 3.3.2 Distributions of Select Variables
As the standard descriptive characteristics of a particular variable are considered (i.e. measures of frequency, central tendency, dispersion, and position), the matrix of density plots below give us a good overview of the most relevant variables in this data set. 

<br>
<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Select_var_summary.png){width=100%}
</center>

##### 3.3.3 Price Index
Under Construction

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Corona General/Sample_Index.png){width=90%}
</center>



<br><br>

### 4. Methodology
The overarching method used in this thesis is the Hedonic Pricing Method (HPM), also often referred to as hedonic regression or hedonic demand theory. The fundamental theory behind the HPM is the following: commodities are distinguishable by their component parts, therefore, the market value of a given commodity can be calculated by summing the estimated values of its separate characteristics. For this theory to hold true, several critical requirement must be met. Primarily, that the commodity being valued can be reduced down to it's component parts and that the market is able to implicitly and independently value these characteristics. The fulfillment of these requirements are not obvious and in reality will in some measure fall short of accounting for the complete nature of price dynamics in practically every asset class. However, this limitation offers an interesting problem to test. Namely, to find the limit of the accumulated power of these component parts to account for market values and their deviations across time and subgroups. This exact questions will be later examined by implementing a machine-learned predictive model to measure the theoretical maximum explanatory power of the included hedonic variables. In the following two subsections, we review the methods used in this paper to econometrically model the HPM on hedonic real estate data.

<br>

#### 4.1 Multi-Variable Linear Regression 
In this section, I will outline the construction of my base OLS model, termed the Alpha model, as well as the treatment process for heteroskadasticity, multicolinearity, non-linearity, and high-leverage points and outliers.  

##### 4.1.1 Basic Model Design

Following the OLS construction laid out by @Herath1:

$$
R = f(P,N,L,t)
$$
where $R$ is rent or price of the property; $P$ is property related attributes; $N$ is neighborhood characteristics; $L$ is locational variables and; $t$ is an indicator of time.

This paper's base OLS model, named the Alpha model, is as follows:

$$
{P}_{n\times1} = \ A_{n\times1} \ + \  B_{k\times1}{V}_{n\times k} \ +  \ \mathcal{E}_{n\times1}
$$

where $P$ is a ${n\times1}$ vector of sold prices; $A$ is a ${n\times1}$ vector of the model's intercepts; $B$ is a ${k\times1}$ vector of beta coefficients; $V$ is a ${n\times k}$ matrix of all hedonic variables; $\mathcal{E}$ is a ${n\times1}$ vector of the model's random error; subscript $n$ is the number of observations and; $k$ is the length of the variable list.

<br>

##### 4.1.2 Accounting for Heteroscadasticity
A Breusch-Pagan test was conducted on a standard linear regression model with sold price as the dependent variable and the rest of the dataset as regressors. The Breusch-Pagan (BP) test was established as a method in 1979 and follows the logic set by the Lagrange multiplier test principle [@Breusch]. This test tests the null hypothesis that the variance in the model's errors is independent from model's regressors (i.e. heteroscadasticity). The test's results in a rejections of the null hypothesis, thereby finding the base model to be heteroskedastic. The results of this test are summarized in *table x*.

<br>

```{r echo=FALSE, warning=FALSE}
df <- suppressMessages(read_excel("/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Tables/BP_Test.xlsx"))
#df <- df[1,1]

table1 <- gt(df)
table1 <- table1 %>% tab_header(title = md("Breusch Paga Test for Heteroskedasticity"),
                                subtitle = md("*Hypotheses*")) %>%
                                tab_style(style = "padding-top:12px;padding-bottom:12px;",
                                locations = cells_column_labels()) %>%
                                tab_spanner(label = "Hypotheses",
                                            columns = c(Hypotheses)) %>%
                                tab_spanner(label = "Test Summary",
                                            columns = c("Test Summary", "-"))
table1
```

<br>

To resolve the heteroskadasticity found in §3.4.1, I will produce heteroskedasticity-consistent (HC) standard errors, also known as heteroskedasticity-robust standard errors, through the refined method established by econometrician Halbert Lynn White [@White].This process is as follows:

If the model's errors $u_{i}$ are independent, but have distinct variances $\sigma _{i}^{2}$ then $\Sigma =\operatorname{diag}(\sigma _{1}^{2},\ldots ,\sigma _{n}^{2})$ which can be estimated with ${\displaystyle {\widehat {\sigma }}_{i}^{2}={\widehat {u}}_{i}^{2}}$. This relationship produces the estimator found in @White:

<br>

$$ {\displaystyle {\begin{aligned}v_{\text{HCE}}\left[{\widehat {\beta }}_{\text{OLS}}\right]&={\frac {1}{n}}\left({\frac {1}{n}}\sum _{i}X_{i}X_{i}'\right)^{-1}\left({\frac {1}{n}}\sum _{i}X_{i}X_{i}'{\widehat {u}}_{i}^{2}\right)\left({\frac {1}{n}}\sum _{i}X_{i}X_{i}'\right)^{-1}\end{aligned}}}$$
$${\displaystyle {\begin{aligned}&=(\mathbb {X} '\mathbb {X} )^{-1}(\mathbb {X} '\operatorname {diag} ({\widehat {u}}_{1}^{2},\ldots ,{\widehat {u}}_{n}^{2})\mathbb {X} )(\mathbb {X} '\mathbb {X} )^{-1},\end{aligned}}}$$

<br>

where $\mathbb{X}$ denotes the matrix of stacked $X_i'$ values from the data. The estimator can be derived in terms of the generalized method of moments (GMM).

For the remainder of this thesis, all statements and figures regarding statistical significance will be referring to tests conducted with heteroskedasticity-consistent (HC) standard errors. As sample errors in my models will have equal variance and are uncorrelated, the least-squares estimates of each model's beta coefficients are regarded as Best Linear Unbiased Estimators (BLUEs).

<br>

##### 4.1.3 Accounting for Multicolinearity
Multicolinearity is measured using Variance Inflation Factors (VIF). The VIF of a predictor measures how accurately that variable can be predicted using all other variables. For context, the square root of a VIF represents the increase in standard error of the estimated coefficient with respect to the case when that given variable is independent of all other variables. Inline with current convention, all variables with a VIF larger than 5 are eliminated. A graphical representation of all variable multicolinearity, measured by VIF, is shown in image x.

This test resulted in the elimination of the variables ***list_price*** and ***area_total*** and there were highly multicolinear with ***sold_price*** and ***area_living*** respectively.

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Multi_colin/Multi_co_combined.png){width=80%}
</center>

<br>

##### 4.1.4 Accounting for Non-linearities
I visual analysis was conducted on all continuous variables and non-linear variables transformation were added to ***age*** and ***area_living***

###### 4.1.4.1 Age
An analysis of age vs. sold price shows a well-established u-shaped pattern. In order to allow the OLS model to better capture this relationship, a new variable $age^2$ is added to the model. 

Image x shows the Partial Dependency Plot (PDP) of age within the Alpha model. This plots the marginal prediction of the Alpha model across the full range of age. When the scale of the y-axis is reduced, we see the slight curvature in Alpha model's estimation of age effects. This addition improved $R^2$ by $.08$. 

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Nonlinearities/age.png){width=70%}
</center>
<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Nonlinearities/age.fitted.png){width=70%}
</center>

###### 4.1.4.2 Living Area
An analysis of living-area vs. sold price reveals a Sigmoid pattern between the two variables. In order to allow the OLS model to better capture this relationship, a new variable $living \ area^2$ is added to the model. 

Image x shows the Partial Dependency Plot (PDP) of $living \ area^2$ within the Alpha model. This addition improved $R^2$ by $.06$.

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Nonlinearities/living_area.png){width=70%}
</center>
<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Nonlinearities/living_area_combined.png){width=70%}
</center>

##### 4.1.5 Accounting for High-Leverage Points and Outliers
After the adjustments or the previous sections have been made, a panel of visualizations are run on the Alpha model. The results show no extreme outliers and only one high-leverage point (obs #23515), as shows by the residual vs. leverage plot in quadrant two of image X, which is removed in the final Alpha model. These results are mainly due to the previous removal of outliers in §3.2 and the overall quality of the data set. 

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/alpha_plots.png){width=70%}
</center>

##### 4.1.6 Final Alpha Model
Under Construction

The Alpha model is the baseline OLS for this thesis and is rebust to heteroskadasticity, multicolinearity, non-linearities, high-leverage points and outliers. These high-level adjustment increase our confidence in the statistical tests results which follow  

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Alpha_model_final.png){width=70%}
</center>


#### 4.3 Modeling Changes in Demand for Hedonic Features
The focus of this thesis is how the Covid crisis impacted housing prices and the relative levels of demand for specific hedonic features. In the case where the HPM is in the OLS functional form, the beta coefficients of this model represent relative demand for each associated hedonic feature [@Shimizu]. For example, $\beta_{pool = 1} =11,856$ is interpreted as the average consumer's willingness to pay for an average pool, ceteris parabus. However, this thesis wishes to measure the *changes* in the average consumer's willingness to pay for a given feature (e.g. pool) in relationship a measurement of Covid's economic impact. 

##### 4.3.1 Comparison Method
A method of statistically comparing changes in the beta coefficients of particular features of interest under multiple scenarios (e.g. post and pre-corona period) is needed. To accomplish this, a method outlined by the UCLA Statistics department is implemented [@Bruin]. The best way to understand this method is to see a simplest reproducible example.

Suppose we want to test the economic impact of Corona on the relative demand for swimming pools, $\beta_{pool = 1}$ and $pool$ respectively. Using the UCLA method, we test the null hypothesis $H_0:\beta_{pool = 1,\ corona = 0} = \beta_{pool = 1,\ corona = 1}$ with the following OLS

<br>

$$
{sold \ price} = \alpha \ + \beta_1 {pool} \ + \  \beta_2 {corona} \ + \beta_3 {(pool \times corona)}
$$
<br>

Which results in the following:

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/UCLA_output_1.png){width=50%}
</center>

<br>

The interpretation of this simplified model's estimates are:

* Intercept: Intercept for $pool = 0$ 
   + The omited group
* pool: Slope for $pool = 1$
   + The included group
* infections period: $($Intercept $infections \ period = 1)$ $-$ $($intercept $infections \ period = 0)$
   + This can be seen by running individual regressions for each case
* (pool*infections period): Slope for $pool_{infections \ period = 1}$ $-$ $pool_{infections \ period = 0}$ 
   + This estimate tests the null hypothesis $H_0: \beta_{pool = 1,\ corona = 0} = \beta_{pool = 1,\ corona = 1}$

<br>

Therefore, we say:

> The average premium for a property having a **swimming pool** fell by **$7,766.40** when compared to pre-corona levels, ceteris parabus. However, this finding is only significant at the ***p < 0.10*** level.  

<br>

##### 4.3.2 Selecting a Corona Measurment
A good measurement variable for measuring the response of the market to the corona crisis must be a corona-related matric which is publicly available; common knowledge to the population and; is a reasonable measurement of future economic shifts. For this reason, data collected from the Louisiana Department of Health [@LaDH] was used to calculate the 3-month moving average of corona infections (infections_3mma).

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Corona_General/Waves_of_infection.png){width=70%}
</center>

<br>

This measurement fulfills the previously stated requirements of a good test measurement as it is purely related to the corona crisis, is publicly available, is assumed to be publicly known as it is reported across all major news stations daily, and perhaps most importantly, is the primary metric used to decide when mandatory lockdowns are instituted. For this thesis, it is assume the market is responding to some lagged value of daily infections which are being used by consumers to estimate the likelihood of future lockdowns and the stringency, and duration of current lockdowns. With this rational, infections_3mma is selected as the primary measurement of corona's impact. 

<br><br>

#### 4.4 Machine Learning 

##### 4.4.1 Machine Learning Methods

In the previous subsection 4.1, it was stated that the multivariable regression models estimate a vector of parameters (i.e. beta coefficients) that best fit the explanatory hedonic variables to the associated dependent variable. Intuitively, the resulting fitted coefficient vector is fitted to the entire data set, and therefore, the loss function minimizes the error in the model's ability to *explain* the very independent variable it was fitted to. In other terms, these results are ultimately limited to their inferential value within the exact context of the data set the model is trained on. If one is to establish a wider, more general relationship between dependent and independent variables that go beyond the context of the trained data set, supervised machine learning (ML) prediction models are an extremely powerful tool to do so. Though the models used in this thesis differ across several key processes, they each generally follow a similar logic: 

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/ML_Process.png){width=70%}
</center>

<br>

##### 4.4.2 Model Evaluation with Cross-Validation

In order to rank order models, we perform a process called Cross-Validation (CV). First, the full data set must be split into 'test' and 'train' (i.e. validation) subsets. Each ML model will be fitted to the train data set and it's performance will be evaluated based the model's ability to predicted out-of-sample observations in the test (validation) data set. In this way, these models are ranked based on their test mean squared errors (MSE). This process is often referred to as Cross Validation (CV). The two most commonly used CV methods are the Validation Set Approach and K-Fold Cross Validation. 

The Validation Set Approach (VSA) is the simplest case of cross validation data splitting as it randomly splits the entire data set into train and test subsets based on a certain percentage split. For example, the researcher can choose to split the data set with a 80% training and 20% testing split. The estimation for the test MSE is simply the test error against the test data set.

<br>

<center> *Test MSE Estimation Equation* </center>

$$CV_{vsa} = MSE_{vsa}$$

<br>

<center>

![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Val_Sets.png){width=50%}
</center>

<br>

The K-Fold CV method has increasingly been used by researches as it offers a more comprehensive cross validation process when compared to other methods. K-Fold CV is the process of randomly splitting the entire data set into k groups, or folds, each with approximately an equal number of observations. The first fold is held out and the model is trained on the remaining k-1 folds. This process is repeated k times, each time holding out a different fold until every fold has been treated as the validation set. Finally, this will results in k estimations of the model's test error and the final estimation will be the average across all k model fits. 

<br>

<center> *Test MSE Estimation Equation* </center>

$$CV_{k-fold} = \frac{1}{k}\sum_{t=1}^{k} MSE_i$$

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/K-Fold.png){width=50%}
</center>

<br>

##### 4.4.4 Model Selection 
For this thesis, ML models will be ranked based on two features: Accuracy, as measured by test MSE, and interpretability, qualitatively defined by the model's ability to provide insights into to which features are relevant to the ability to make correct predictions, and by how much are they relevant.

*Accuracy*

Since a method of model fitting and evaluation has been established in the previous section through the process of Cross Validation, we now have a way to rank different models to each other based on their ability to estimate test MSE. With this feature, it is possible to compare several models to one another in terms of effectiveness in predictions. Five different ML models were build for this thesis with the following results:

<br>

```{r echo=FALSE, results='asis'}
# Cleaned Data Set Summary
df3 <- suppressMessages(read_excel("/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Tables/ML_Results.xlsx"))

table1 <- gt(df3)
table1 <- table1 %>% tab_header(title = md("Variable List"),
                                subtitle = md("*Structure and short discription*"))

table1
```

<br>

*Interpretability*

A major criticism of ML models are their lack of interpretabilty, explaining *how* the model makes such accurate predictions. A prime example of this critique can be seen in Artificial Neural Networks (ANN) machines, which often use a large number of small nodes to sequentially generate a single prediction without any one of these nodes holding a clear interpretation as to which variables, or combination of variables, lead to a particular improvement in prediction. However, as ML methods become more commonly used, the demand for interpretation of these models has driven several useful interpretation methods across for a variety of models. These methods include interpretation for a single prediction, such as the Local Interpretable Model-agnostic Explanations (LIME), as well as generalized methods of measuring relative feature importance, which uses various techniques to determine the average contribution of each variable to the model's ability to decrease its test MSE rate. 


Of the models sampled, the eXtreme Gradient Boosting Machine, also referred to as XGBoost, outperforms the other models both in terms of having the lowest test MSE and having the best methods for detailed variable interpretation. For this reason, XGBoost is chosen as the primary ML model for this thesis 


<br>

##### 4.4.5 eXtreme Gradient Boosting Machine Algorithm
In order to understand the logic of XGBoost, one must first look at the compact, yet powerful algorithm behind its computation. In this section, I will lay out the mathematical formulation of the input, core algorithm, and final output of the XGBoost machine.

<br>

1. Algorithm input: We start with a training set ${\displaystyle \{(x_{i},y_{i})\}_{i=1}^{N}}$, a differentiable loss function $L(y, F(x))$, a number of weak learners (shallow trees) $M$, and a learning rate $\alpha$.


2. Algorithm:

2.1 Initialize model with a constant value:

$${\displaystyle {\hat {f}}_{(0)}(x)={\underset {\theta }{\arg \min }}\sum _{i=1}^{N}L(y_{i},\theta )}$$

<br>

2.2 For $m = 1$ to $M$:

2.2.1 Compute the 'gradients' and 'hessians
$${\displaystyle {\hat {g}}_{m}(x_{i})=\left[{\frac {\partial L(y_{i},f(x_{i}))}{\partial f(x_{i})}}\right]_{f(x)={\hat {f}}_{(m-1)}(x)}}$$
$${\displaystyle {\hat {h}}_{m}(x_{i})=\left[{\frac {\partial ^{2}L(y_{i},f(x_{i}))}{\partial f(x_{i})^{2}}}\right]_{f(x)={\hat {f}}_{(m-1)}(x)}}$$

<br>

2.2.2 Fit an original weak learner (e.g. shallow tree) using the training set ${\displaystyle \displaystyle \{x_{i},-{\frac {{\hat {g}}_{m}(x_{i})}{{\hat {h}}_{m}(x_{i})}}\}_{i=1}^{N}}$ by solving the following optimization problem:

$${\displaystyle {\hat {\phi }}_{m}={\underset {\phi \in \mathbf {\Phi } }{\arg \min }}\sum _{i=1}^{N}{\frac {1}{2}}{\hat {h}}_{m}(x_{i})\left[-{\frac {{\hat {g}}_{m}(x_{i})}{{\hat {h}}_{m}(x_{i})}}-\phi (x_{i})\right]^{2}}$$

$${\displaystyle {\hat {f}}_{m}(x)=\alpha {\hat {\phi }}_{m}(x).}$$

<br>

2.3 Update the model:
$${\displaystyle {\hat {f}}_{(m)}(x)={\hat {f}}_{(m-1)}(x)+{\hat {f}}_{m}(x).}$$

<br>

3. Algorithm Output: 
$${\displaystyle {\hat {f}}(x)={\hat {f}}_{(M)}(x)=\sum _{m=0}^{M}{\hat {f}}_{m}(x)}$$

<br>

##### 4.4.6 XGBoost Model Fitting and Hyperparameter Tuning
In practice, when attempting to produce the most optimal results from an XGBoost machine, as is true with most other ML models, one must first transform the data set into an optimal form and then run set of hyperparameter tests to determine the appropriate level for each of the model's basic structural rules (i.e. hyperparameters).

Since the XGBoost machine requires only numerical data, factor data must be converted into numerical levels which can be the used in the construction of decision trees. To accomplish this with the added complexity of some factor variables having more than two levels, I have used a method called *One-Hot Encoding*, which encodes each individual factor variable level into a vector containing '1' if that factor and level are present, and '0' if it is not. In this way, the dataframe is converted into a large matrix of continuous and binary columns.

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/One-Hot_Encoding.png){width=90%}
</center>

<br>

Once the model is fitted to the data, the next procedure is to tune the hyperparameters which govern the algorithm's learning process and therefore determine the resulting values of estimated parameters. To do this, a large grid of hyperparameters is created and then an individual model containing each unique combination of hyperparameter levels is generated and their resulting test MSE's are ranked and analyzed. The results from this large grid search determine which combination of hyperparameters are optimal, and those hyperparameters are used in the final model. The search grid from this research was so large, it took my computer *78 hours* to complete all of the calculation necessary for the full tuning process.

<br>

##### 4.4.7 XGBoost Partial Dependency Plots

Partial dependency plots (PDP) shows relationship, or dependence, between the model's response variable (i.e. $sold \ price$) and a chosen variable, or set of variables, of interests (VoI), resulting in the graphical representation of a variables marginal contribution to the machine's prediction across the variable of interest's entire range. In order to analyze the results of the XGBoost machine at the variable-by-variable level, I have generate a panel of four graphical partial dependency plots for each variable of interest. Thesis will be the following: Basic PDP plot, Individual Conditional Expectation (ICE) plots, PDP heatmap with VoI against corona infection, 3-dimensional PDP heatmap with VoI against corona infection. 

Image XX shows an example of each graph 

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/PDP_Example_Combined.png){width=99%}
</center>

<br>

<br><br>

### 5. Hypothesis Construction

#### 5.1 Covid-19: General Case
> **Hypothesis I:** The Covid-19 crisis significantly increased housing prices
>
> **Reasoning:** As many workers have perminantly shifted to remote work, the total utility of residential housing has increased, thereby increasing the price households are willing to pay. This price shift can be explained through measuring the changes if relative demand for specific hedonic features, such as bedrooms, size, age, and others. 

<br>

#### 5.2 Covid-19: Premium for Bedrooms
> **Hypothesis II:** The Covid-19 crisis significantly increased demand-premiums for every level of bedrooms 
>
> **Reasoning:** As many workers have perminantly shifted to remote work, the premium for an additional bedroom is expected to increase across each level of total_beds as households need additional rooms for homeoffices and other activities.  

<br>

#### 5.3 Covid-19: Premium for City Centrality
> **Hypothesis III:** The Covid-19 crisis impacted properties within city limits more than those not within city limits  
>
> **Reasoning:** As workers who live in city limits have jobs more likely to be compateble with remote work, such as jobs within the financial industry, home prices within city limits are disproporsionatly impacted by the shift to remote work.

<br>

#### 5.4 Covid-19: Premium for Relative Price Levels 
> **Hypothesis IV:** The Covid-19 crisis impacted properties in the top 25th percentile of price more than the bottom 25th percentile 
>
> **Reasoning:** Households who live in the top 25th percentile of home prices are more likely to hold white-collar jobs, which are more likely to be made remote, which increases the premium willing to pay for additonal hedonic features.  

<br>

#### 5.5 Covid-19: Premium for Size

> **Hypothesis V:** The Covid-19 crisis increased the premium for property size 
>
> **Reasoning:** Same as for hypothesis I

<br>

> **Hypothesis VI:** The Covid-19 crisis impacted properties in the top 25th percentile of price more than the bottom 25th percentile 
>
> **Reasoning:** Households who live in the top 25th percentile of home sizes are more likely to hold white-collar jobs, which are more likely to be made remote, which increases the premium they are willing to pay for additonal hedonic features.

<br>

#### 5.6 Covid-19: Premium for Age 

> **Hypothesis VII:** The Covid-19 crisis increased the premium for property age 
>
> **Reasoning:** As the volocity of home sales increase, the general relationship between property age and price is expected to be exasterbated as those upgrading houses will shit towards newer properties

<br>

> **Hypothesis VIII:** The Covid-19 crisis impacted properties in the top 25th percentile of age more than the bottom 25th percentile  
>
> **Reasoning:** The premium for the youngest (i.e. newest) properties is expected to increase while the premium for the oldest properties is expected to decrease. As the market shifts towards newer properties, they must necissarily shift away from older properties.

<br>

#### 5.7 Covid-19: Change in Days on Market

> **Hypothesis IX:** The Covid-19 crisis significantly impacted the premium for days on market 
>
> **Reasoning:** As the volocity of home sales increase, more homes are sold faster, and therefore the number of days on the market becomes less of a predictor of quality since even lower quality and over-priced homes are sold more quickly. 

<br>

> **Hypothesis X:** The Covid-19 crisis impacted properties in the top 25th percentile of days on market more than the bottom 25th percentile 
>
> **Reasoning:** The premium for a property being sold within the top 25th percentile of days on market (DOM), i.e. the properties which sit on the market the logest, is expected to be disproportionately effected when compared homes which sold the fastest, as households elasticity of demand for specific characteristics decreses relative to price, it will take larger deviation in price and quality to make a home sell in the top 25th percentile of DOM 



<br><br>

### 6. Results
This section will follow a clear and consistent structure for each subsection of results. Each hypothesis will have: A *general overview* of the characteristic being modeled and; *model results* with standard hypothesis conclusion(s). 

<br>

#### 6.1 Covid-19: General Case
##### 6.1.1 Summary of Findings
> Preliminary analysis of housing prices and daily infections reveals a positive historical relationship between these two key variables. This relationship is strongly supported by the XGBoost ML algorithm, which shows that a maximal reduction in predictions error is acheived by increasing the prediciton price at all levels of daily infections > 0, holding all other factors constant. XGBoost determines that from the 104 total variables, daily infections is the 19th most important variable in reducing price prediction error.  
>
> Furthermore, in the fully controlled Alpha model, the beta coefficient for daily infections suggests that each additional infection is associated with an average home price *increase of $8.97*, ceteris parabus. This finding is significant at the *p < 0.00* level.  

##### 6.1.2 Visual Review
We first look at the distribution of daily infections and the accumulation of infections across time. The variation in historical daily infections should provide a strong measurement for explaining variations in price related to this key variable. 

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Corona_General/Infections_combined.png){width=99%}
</center>

<br>

Looking at the raw historical relationship between infections rates and prices, we look at the trend line of price vs daily infections (rhs) and a comparison between the price distributions of pre and post-infections period (lhs), we see historically positive relationship between infections and price. Though these graphs are promising, it is unclear if other factors could be influencing this relationship. To establish  

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Corona_General/Infect_price_combo.png){width=99%}
</center>

<br>

##### 6.1.3 OLS Modeling
Since displaying the full OLS output for ever section is impractical, I will include a summarized version with the only the key variables included. Please note that if you wish to see each regression output table, code, and data associated with this thesis, you can visit my GitHub repository with the following link: https://github.com/Sawbenson15/HPM_Thesis.

> In the fully controlled Alpha model, the beta coefficient for daily infections shows that each additional infection is associated with an average home price *increase of $8.97*, ceteris parabus. This finding is significant at the *p < 0.00* level   

This foundational finding lays the groundwork for the following results, which attempt to explain the significant relationship between infections and price at the level of individual hedonic variables.  

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Corona_General/summary_OLS.png){width=40%}
</center>

<br><br>

The following graphic shows the historical trend line for infection vs sold price with 1SD error margins (grey), the best single-variable fit (green), and the marginal fit of the Alpha model (blue). We see that the controls of the Alpha model have flatten the marginal effect of each additional infection on price, however, this relationshiip remains positive and significant, as previously established.  

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Corona_General/Corona_price_OLSFit.png){width=55%}
</center>

<br>

##### 6.1.4 ML Modeling
To further test the relevance of corona infections in determining prices, we look at this variable's relative ranking based it's ability to improve out-of-sample predictions within our XGboost model. This measurment is called Variable Importance. 

We see that of the 104 unique variables offered to the XGBoost machine, the 3-month moving average of infection was the 19th most descriptive variable in predicting prices. This is an very strong confirmation that the relationship found in the OLS model is correct. 

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Corona_General/Var_Importance.png){width=90%}
</center>

To get a detailed understanding of the marginal effect each additional infection has on price through, we look at the PDP and the ICE of infections and price. The PDP (top) shows the optimal price change w.r.t. daily infections which minimize the model's test MSE. The ICE (bottom) is the individual PDP for every observation in the dataset, centered at infection = 0. This allows us to understand the simple PDP in more detail and also to pick up on any signs of heteroskedasticity amoung individual sample predictions. 

The PDP shows that on average, an increase in price at every level of daily infections > 0 reduces test MSE. Furthermore, this trend is generally upward facing, with a notable range of response between 1,800 and 2,500 infections, which I refered to as the *infections-price ridge.* Discussions regarding possible explanations for the general shape of this region will be explored in section 7.     

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Corona_General/PDP_ICE_general.png){width=80%}
</center>

<br>

#### 6.2 Covid-19: Premium for Bedrooms
##### 6.2.1 Summary of Findings

> There are two contextual details which must be considered for one to appropriatly interpret the following findings. First, number of bedrooms and the total living area of a home are highly correlated, and therefore when total living area is included, the beta coefficients for each level of bedroom represents only the premium homeowners are willing to pay for the actual feature of an extra bedroom, and not the extra living area itself. This will make premiums seem low. Secondly, the OLS results listed below represent the change in the beta coefficients from post and pre corona, not the absolute effect, as in the case of measuring daily infection alone.     
>
> Prelimilary finding show that the price distribution for every level of number of bedrooms increased after the beginning of the infection period (i.e. accumulation of infections >= 1000)
>
> The fully controled Alpha model results show that the premium for each level of number of bedrooms modestly increased in response to daily infection. Every corona-driven increase is significant at the p < 0.05 level except for single bedroom homes. Possible theories for why this shift happened is explored in section 7.


##### 6.2.2 Visual Review
General distribution of bedrooms per house

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Beds/bed_dist_gen.png){width=80%}
</center>

<br>

Distributions of price at each number of bedrooms (lhs) and the same plot but split or before and after infections period (rhs)

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Beds/bed_combined.png){width=100%}
</center>

<br>

However, it is important to remember that size (i.e. total living area) is controlled for in the Alpha model. Therefore, it is more accurate to look at the same two plots above, but normalized per square foot. We see a significant flattening between each number of bedrooms level, however, a distinct change from pre vs post infection period is still visible (rhs). The next section will test if this shift from post to pre-corona is significantly in general, and significantly related to daily infections.

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Beds/bed_ppsf_combined.png){width=100%}
</center>

<br>

##### 6.2.3 OLS Modeling
Interpretation under construction

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Beds/OLS_Output.png){width=50%}
</center>

<br>

##### 6.2.4 ML Modeling
Due to number of bedrooms being categorical, the results of the XGBoost model for these variables are not as easily visually represented as continuous variables, which have celar PDPs. However, we can consider the results in written form and with importance ranking. 

The variable importance ranking of each factor level from 1-through-5 bedrooms is 84, 60, 47, 34, and 68 respectively. This relatively low relevance is expected, as much of the explanatory power of bedrooms on price is taken by total living area, ranked number 1 in importance.

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Beds/beds_importance.png){width=60%}
</center>

<br>

#### 6.3 Covid-19: Premium for City Centrality
##### 6.3.1 Summary of Findings



##### 6.3.2 Visual Review
<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/City/total_dist.png){width=100%}
</center>

<br>
<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/City/city_dist.png){width=100%}
</center>

<br>
<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/City/city_limits_and_corona.png){width=80%}
</center>

<br>

##### 6.3.3 OLS Modeling
<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/City/OLS.png){width=50%}
</center>

<br>


##### 6.3.4 ML Modeling
<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/City/City_PDP.png){width=50%}
</center>

<br>

#### 6.4 Covid-19: Premium for Size
##### 6.4.1 Summary of Findings



##### 6.4.2 Visual Review
<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Living_area/area_dist_total_com.png){width=100%}
</center>

<br>
<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Living_area/area_dist_price_coronaa.png){width=70%}
</center>

<br>

##### 6.4.3 OLS Modeling

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Living_area/area_total.png){width=50%}
</center>

<br>

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Living_area/Screen Shot 2022-02-06 at 18.36.46.png){width=100%}
</center>

<br>

##### 6.4.4 ML Modeling
<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Living_area/area_PDP.png){width=90%}
</center>

<br>

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Living_area/area_heats_com.png){width=90%}
</center>

<br>


#### 6.5 Covid-19: Premium for Age 
##### 6.5.1 Summary of Findings
##### 6.5.2 Visual Review
<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Age/age_dists_com.png){width=90%}
</center>

<br>

##### 6.5.3 OLS Modeling
<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Age/age_gen.png){width=50%}
</center>

<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Age/age_ols_com.png){width=100%}
</center>

<br>

##### 6.5.4 ML Modeling
<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Age/age_PDP.png){width=100%}
</center>

<br>
<br>

<center>
![](/Users/sawyerbenson/Documents/Master Thesis/HPM_Thesis/Writing & Literature/Graphics from pptx/Results/Age/age_PDP.png){width=100%}
</center>

<br>

#### 6.6 Covid-19: Change in Days on Market
##### 6.6.1 Summary of Findings
##### 6.6.2 Visual Review
##### 6.6.3 OLS Modeling
##### 6.6.4 ML Modeling


#### 6.7 Summary of Results and Hypothesis 




<br><br>

### 7. Discussion

<br><br>

### 8. Conclusion

<br><br>

### 9. Bibliography

<br><br>

### 9. Appendix

<br><br>

#### General Notes and Todos
 
 * Possibly write in LaTex?
 * Just write cover letter in word and combine PDF?
 * Complete general Structure
 * Select system for managing references
 * Get confirmation on title change from Prüfungsamt
 * Possible additional data
   + Include Covid cases in La by date?
   + Crime stats?
   + Racial Stats?
 * Include the terms
   + Big Data
 * Build some graphics in pptx and import them as images
 * Web scrapping for additional data sets?
 * Graph with **DATE** as variable
 * Make German study in German
 * Replicate with other data sets
 * Check for plagiarism 
 * Make video explaining thesis for prof. grading

#### Helpful Notes

1. Define the central concepts used early on
2. Start each chapter with a small introduction and end it with a brief summary of results
3. Provide an outlook of the future developments of the particular scientific discussion, discuss policy implications and point out open questions.

#### Playground

```{r}

```


<br><br><br>

End of Document


